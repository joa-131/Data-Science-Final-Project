{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import emoji\n",
    "import numpy as np\n",
    "# from googleapiclient.discovery import build\n",
    "from google.cloud import language\n",
    "from google.cloud.language import types\n",
    "from google.cloud.language import enums\n",
    "# import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# authorize tweepy\n",
    "consumer_key = \"OROBANkVOsvva9HETWL4Kovbx\"\n",
    "consumer_secret = \"t87379Kk9ANccJDM5a6E6G5eLKDGLnL7s2zld1kXvUCYu9gjnJ\"\n",
    "access_token = \"1305689531421786112-FNe1D2tjmvFjzikZYHDyjvRVAVT9gk\"\n",
    "access_token_secret = \"G6KV4pfTg6l7BSXxbQZDyYbREmr2taNkYzBzSZZfDyBIa\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit = True)\n",
    "\n",
    "languages = [\"ar\",\"zh\",\"nl\",\"en\",\"fr\",\"de\",\"id\",\"it\",\"ja\",\"ko\",\"pt\",\"es\",\"th\",\"tr\"]\n",
    "\n",
    "client = language.LanguageServiceClient.from_service_account_json \\\n",
    "    (\"/Users/phoenix/Documents/1stSemesterSO/DSCI125/Projects/Data-Science-Final-Project/DSCI_Auth.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HFI Feature Extraction - Capture Step\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "def get_coordinates(filename):\n",
    "    lat_long = pd.read_csv(filename,usecols = [\"latitude\",\"longitude\",\"country\"])\n",
    "    lat_long[\"country\"] = lat_long[\"country\"].astype(str).apply(lambda s: s.strip())\n",
    "\n",
    "    radius = pd.read_csv(\"countries of the world.csv\",usecols = [\"Country\",\"Area (sq. mi.)\"])\n",
    "    radius[\"Country\"] = radius[\"Country\"].astype(str).apply(lambda s: s.strip())\n",
    "    radius[\"Area (sq. mi.)\"] = radius[\"Area (sq. mi.)\"].astype(float).apply(lambda x: math.sqrt(x))\n",
    "    radius.columns = [\"country\",\"area\"]\n",
    "    radius.columns = [\"country\",\"radius\"]\n",
    "\n",
    "    coordinates = pd.merge(lat_long,radius,on = [\"country\"])\n",
    "\n",
    "    coordinates[\"lat-long-radius\"] = coordinates[\"latitude\"].astype(str) + \",\" + \\\n",
    "                                     coordinates[\"longitude\"].astype(str) + \\\n",
    "                                     \",\" + coordinates[\"radius\"].astype(str) + \"mi\"\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "def read_csv_hfi(filename):\n",
    "    df_byCountry = pd.read_csv(filename,usecols = [\"countries\",\"region\",\"hf_score\",\"hf_rank\",\"hf_quartile\",\n",
    "                                                   \"pf_expression\",\"pf_association_assembly\",\"pf_movement\"],na_values = [\"-\"])\n",
    "\n",
    "    df_byCountry.loc[:,\"hf_score\":\"pf_movement\"] = df_byCountry.loc[:,\"hf_score\":\"pf_movement\"].astype(float)\n",
    "\n",
    "    aggregation_functions = {\"region\": \"first\",\"hf_score\": \"mean\",\"hf_rank\": \"max\",\"hf_quartile\": \"max\",\n",
    "                             \"pf_expression\": \"mean\",\"pf_association_assembly\": \"mean\",\"pf_movement\": \"mean\"}\n",
    "\n",
    "    df_byRegion = df_byCountry.groupby(by = [\"countries\"]).aggregate(aggregation_functions)\n",
    "\n",
    "    return df_byCountry,df_byRegion\n",
    "\n",
    "\n",
    "# Twitter Interaction and Feature Extraction - Capture Step\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "def filter_regex(text):\n",
    "    text = re.sub(r\"(?:\\#|\\@|https?\\:(\\/\\/))\\S+\",\"\",text)\n",
    "    text = re.sub(r\"\\w*(RT|rt)\\w*\",\"\",text)\n",
    "    text = re.sub(r\"(<[^>]+>)\\S+\",\"\",text)\n",
    "\n",
    "    text =  emoji.get_emoji_regexp().sub(u'',text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# create get_tweets -> takes coordinates, a datetime range to search, result type, maximum number of tweets to scrape\n",
    "def get_tweets(coordinates,result_type,until_date,count,max_tweets):\n",
    "    \"\"\"Takes in 5 parameters, set the max_tweets to 150, count to 25, and result type to 'recent'\n",
    "    until date as the current date, and the coordinates equal to the 'latitude' and 'longitude'\n",
    "    values. Returns dataframe with following columns -> text, created_at_date, favorite_count,\n",
    "    user location, followers count, friends count, and the language used.\n",
    "    Indexed 0 - 6\"\"\"\n",
    "    tweets = tweepy.Cursor(api.search,geocode = coordinates,result_type = result_type,\n",
    "                           until = until_date,count = count).items(max_tweets)\n",
    "\n",
    "    tweets_list = [\n",
    "         [filter_regex(tweet.text),tweet.created_at,tweet.retweet_count,tweet.favorite_count,\n",
    "          tweet.user.location,tweet.user.followers_count,tweet.user.friends_count,\n",
    "          tweet.lang] for tweet in tweets]\n",
    "\n",
    "\n",
    "    tweets_df = pd.DataFrame(data = tweets_list,columns = [\"text\",\"created_at\",\"retweet_count\",\"favorite_count\",\n",
    "                                                           \"user_location\",\"followers_count\",\n",
    "                                                           \"friends_count\",\"language\"])\n",
    "\n",
    "    return tweets_df\n",
    "\n",
    "\n",
    "def get_tweets_by_country(coordinates_df):\n",
    "    \"\"\"Takes in tweet_df, run the get_tweets to get all those tweets\n",
    "    Uses latitude, longitude, and radius, return a dataframe with each iteration\n",
    "    Save into list of dataframes, and merge the list into a single frame\n",
    "    byCountry -> country, all the columns of tweets_df\"\"\"\n",
    "    # return the dataframe with country, lat, and longitude, radius\n",
    "    result_type = 'recent'\n",
    "    max_tweets = 500\n",
    "\n",
    "    tweets_by_country = pd.DataFrame()\n",
    "\n",
    "    print(f\"retrieving tweets, 5000 tweets remaining\")\n",
    "\n",
    "    # go through the dataframe for the coordinates by country, and retrieve the tweets from each country (~25)\n",
    "    # save results into dataframe with following characteristics -> country, text\n",
    "    for coordinate in coordinates_df.itertuples():\n",
    "        tweets = get_tweets(coordinate[2],result_type,date,150,max_tweets)\n",
    "        tweets[\"region\"] = coordinate[1]\n",
    "        tweets_by_country = pd.concat([tweets_by_country,tweets])\n",
    "        print(f\"retrieving tweets, {5000 - len(tweets_by_country)} tweets remaining\")\n",
    "\n",
    "    tweets_by_country = tweets_by_country[tweets_by_country[\"language\"].isin(languages)]\n",
    "\n",
    "    return tweets_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving tweets, 5000 tweets remaining\n",
      "retrieving tweets, 4500 tweets remaining\n",
      "retrieving tweets, 4000 tweets remaining\n",
      "retrieving tweets, 3500 tweets remaining\n",
      "retrieving tweets, 3000 tweets remaining\n",
      "retrieving tweets, 2500 tweets remaining\n",
      "retrieving tweets, 2000 tweets remaining\n",
      "retrieving tweets, 1500 tweets remaining\n",
      "retrieving tweets, 1000 tweets remaining\n",
      "retrieving tweets, 500 tweets remaining\n",
      "retrieving tweets, 0 tweets remaining\n"
     ]
    }
   ],
   "source": [
    "df,df_aggregates = read_csv_hfi(\"hfi_cc_2019.csv\")\n",
    "df_by_country,df_by_region = read_csv_hfi(\"hfi_cc_2019.csv\")\n",
    "df_coordinates = get_coordinates(\"lat_long_coordinates.csv\")\n",
    "\n",
    "df_regional_coord = pd.read_csv(\"regional_coordinates .csv\")\n",
    "\n",
    "df_tweets = get_tweets_by_country(df_regional_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine Sentiment and Magnitude of Tweets - Process Step$\n",
    "def find_sentiment_score(tweets_text):\n",
    "\n",
    "    tweets = pd.DataFrame(columns = [\"score\",\"magnitude\"])\n",
    "\n",
    "    document = types.Document(\n",
    "        content = tweets_text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects the sentiment of the text\n",
    "    response = client.analyze_sentiment(document = document,encoding_type = \"UTF8\").document_sentiment.score\n",
    "    \n",
    "    return response\n",
    "\n",
    "def find_sentiment_magnitude(tweets_text):\n",
    "\n",
    "    tweets = pd.DataFrame(columns = [\"score\",\"magnitude\"])\n",
    "\n",
    "    document = types.Document(\n",
    "        content = tweets_text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects the sentiment of the text\n",
    "    response = client.analyze_sentiment(document = document,encoding_type = \"UTF8\").document_sentiment.magnitude\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tweet_dataset(tweets):\n",
    "    df,df_aggregates = read_csv_hfi(\"hfi_cc_2019.csv\")\n",
    "    df_by_country,df_by_region = read_csv_hfi(\"hfi_cc_2019.csv\")\n",
    "    df_coordinates = get_coordinates(\"lat_long_coordinates.csv\")\n",
    "\n",
    "    df_regional_coord = pd.read_csv(\"regional_coordinates .csv\")\n",
    "\n",
    "    regions = ['Caucasus & Central Asia', 'East Asia', 'Eastern Europe',\n",
    "     'Latin America & the Caribbean', 'Middle East & North Africa',\n",
    "     'North America', 'Oceania', 'South Asia', 'Sub-Saharan Africa',\n",
    "     'Western Europe']\n",
    "    \n",
    "    by_region_dataframe = pd.DataFrame()\n",
    "    \n",
    "    tracker = 0\n",
    "    for r in regions:\n",
    "        tweet = tweets.loc[tweets[\"region\"] == r].iloc[:100] \n",
    "        by_region_dataframe = pd.concat([by_region_dataframe,tweet])\n",
    "    \n",
    "    return by_region_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = build_tweet_dataset(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 The language ku is not supported for document_sentiment analysis.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    825\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"The language ku is not supported for document_sentiment analysis.\"\n\tdebug_error_string = \"{\"created\":\"@1606779203.626050000\",\"description\":\"Error received from peer ipv4:172.217.11.10:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"The language ku is not supported for document_sentiment analysis.\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c4ea6bcd2fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_sentiment_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c4ea6bcd2fe2>\u001b[0m in \u001b[0;36mfind_sentiment_main\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m83\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m460\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf_sentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_sentiment_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdf_sentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"magnitude\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_sentiment_magnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2366d7dd0d25>\u001b[0m in \u001b[0;36mfind_sentiment_score\u001b[0;34m(tweets_text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Detects the sentiment of the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"UTF8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/cloud/language_v1/gapic/language_service_client.py\u001b[0m in \u001b[0;36manalyze_sentiment\u001b[0;34m(self, document, encoding_type, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    245\u001b[0m         )\n\u001b[1;32m    246\u001b[0m         return self._inner_api_calls[\"analyze_sentiment\"](\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             )\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;34m\"\"\"Wrapped function that adds timeout.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 The language ku is not supported for document_sentiment analysis."
     ]
    }
   ],
   "source": [
    "def find_sentiment_main(tweets):\n",
    "    \n",
    "    df_sentiment = tweets.reset_index()\n",
    "\n",
    "    df_sentiment = df_sentiment.drop([83,460])\n",
    "    \n",
    "    df_sentiment[\"score\"] = df_sentiment.text.apply(find_sentiment_score)\n",
    "    df_sentiment[\"magnitude\"] = df_sentiment.text.apply(find_sentiment_magnitude)\n",
    "\n",
    "    return df_sentiment\n",
    "\n",
    "\n",
    "df = find_sentiment_main(tweets)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sentiment_analysis_byRegion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
